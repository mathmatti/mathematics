\chapter{Applicazioni lineari}
\section{Definizione}
\begin{definizione} \label{d:applicazione-lineare}
	Siano $V$ e $Z$ due spazi vettoriali su un campo $K$.
	Si definisce \emph{applicazione lineare} una funzione $T\colon V\to Z$ omogenea e additiva, ossia per la quale $\forall  x,  y\in V$ e $\lambda\in K$, si ha
	\begin{equation*}
		T(x+y)=T(x)+T(y)\qeq T(\lambda x)=\lambda T(x),
	\end{equation*}
	dove i primi membri delle uguaglianze sono operazioni in $V$, e i secondi sono operazioni in $Z$ (indicando con $+$ e $\cdot$ le due operazioni interne per entrambi gli spazi).
\end{definizione}
Le applicazioni lineari sono dunque degli omomorfismi tra spazi vettoriali, in quanto ne preservano la struttura data dalle operazioni interne.
Per linearità, vale sempre la relazione $T(0_V)=0_Z$, poiché $T(0_V)=T(0_V+0_V)=T(0_V)+T(0_V)$, da cui $T(0_V)=0_Z$ sottraendo $T(0_V)$ a entrambi i membri: quest aè una condizione facile da usare per verificare la linearità di una data funzione.
L'insieme delle applicazioni lineari da $V$ a $Z$ (entrambi basati sullo stesso campo) è indicato con $\lin(V,Z)$.
In esso possiamo definire l'addizione e la moltiplicazione per uno scalare come
\begin{equation*}
	(T+S)(v)\defeq T(v)+S(v),\hspace{2cm}(\lambda T)(v)\defeq \lambda \big(T(v)\big)
\end{equation*}
per $T,S\in\lin(V,Z)$, $v\in V$ e $\lambda\in K$.
Dotato di queste operazioni, $\lin(V,Z)$ è uno spazio vettoriale sul campo $K$.
L'elemento neutro dell'addizione è l'applicazione (evidentemente lineare) che manda ogni elemento di $V$ in $0_Z$, anche detta \emph{applicazione nulla}.

\paragraph{Esempi}
\begin{itemize}
	\item L'applicazione identità di $V$, indicata spesso con $I_V$, per la quale $I_V(v)=v$ per ogni $v\in V$, è ovviamente lineare.
	\item Sia $W$ un sottospazio di $V$: chiamiamo \emph{proiezione canonica} in $W$ la funzione $\pi\colon V\to V\quot W$ che ad ogni $v\in V$ associa la classe di equivalenza $[v]_W=W+v$.
		Tale funzione è additiva in quanto per $a,b\in V$, $\pi(a+b)=W+(a+b)=(W+a)+(W+b)=\pi(a)+\pi(b)$.
		È anche omogenea perch\'e $\pi(\lambda a)=W+\lambda a=\lambda(W+a)=\lambda\pi(a)$.
		La funzione $\pi$ è quindi un'applicazione lineare.
	\item La funzione $f\colon\R^2\to\R^2$ data da $f([x,y]^T)=[x+y,y+1]^T$ non è un'applicazione lineare, poich\'e $f([0,0]^T)=[0,1]^T\ne[0,0]^T$.
\end{itemize}

Un'applicazione lineare da uno spazio in s\'e stesso è detta \emph{endomorfismo} (lineare), e indichiamo l'insieme degli endomorfismi di uno spazio $V$ con $\End(V)$.
In questo insieme possiamo definire un'ulteriore operazione, la composizione tra endomorfismi: con essa $\End(V)$ assume l'ulteriore struttura di \emph{algebra}, sempre sul campo $K$.
L'elemento neutro di $\End(V)$ è l'identità $I_V$, come nell'esempio precedente, che associa ogni elemento di $V$ a s\'e stesso, ed è tale che $I_V\circ T=T\circ I_V=T$ per ogni $T\in\End(V)$.

L'iniettività e la suriettività delle applicazioni lineari sono definite allo stesso modo di una qualsiasi funzione.
In particolare, un'applicazione lineare che è sia iniettiva che suriettiva è detta \emph{isomorfismo lineare} (o solo isomorfismo).
Due spazi vettoriali $V$ e $Z$ legati da un isomorfismo $T\colon V\to Z$ sono detti \emph{isomorfi}, e si indica ciò con la scrittura $V\cong Z$.

Come ogni omomorfismo, definiamo un nucleo anche per le applicazioni lineari.
\begin{definizione} \label{d:nucleo-applicazione-lineare}
	Si definisce \emph{nucleo} dell'omomorfismo $T$ l'insieme degli elementi $x\in V$ tali per cui $T(x)=0_Z$:
	\begin{equation*}
		\Ker T=\{x\in V\colon T(x)=0_Z\}.
	\end{equation*}
\end{definizione}
La linearità permette una facile verifica dell'iniettività di un'applicazione lineare.
\begin{teorema} \label{t:iniettivita-nucleo}
	Un'applicazione lineare $T\in\lin(V,Z)$ è iniettiva se e solo se $\Ker T=\{0_V\}$.
\end{teorema}
\begin{proof}
	Per ogni applicazione lineare vale $T(0_V)=0_Z$, come già visto.
	Per l'iniettività, ogni $z\in Z$ ha un'unica controimmagine, quindi l'unico $v\in V$ per cui $T(v)=0_Z$ è proprio $0_V$, di conseguenza $\Ker T=\{0_V\}$.

	Viceversa, ipotizziamo $\Ker T=\{0_V\}$.
	Se $T(v)=T(w)$ allora $T(v)-T(w)=0_Z$, e per linearità si ha $T(v-w)=0_Z$, cioè $v-w\in\Ker T$.
	Ma $\Ker T=\{0_V\}$ implica $v-w=0_V$, dunque $v=w$ e $T$ è iniettiva.
\end{proof}

\begin{definizione} \label{d:immagine-applicazione-lineare}
	Sia $T\in\lin(V,Z)$.
	Si definisce \emph{immagine} di $T$ l'insieme degli elementi $z\in Z$ per i quali esiste una controimmagine $v\in V$:
	\begin{equation*}
		\Imm T=\{z\in Z\colon\exists v\in V\colon T(v)=z\}.
	\end{equation*}
\end{definizione}
Sia il nucleo che l'immagine di un omomorfismo $T\in\lin(V,Z)$ sono sottospazi vettoriali, rispettivamente di $V$ e di $Z$.
Infatti:
\begin{itemize}
	\item Siano $v,w\in\Ker T$ e $\lambda \in K$.
		Allora $T(v+w)=T(v)+T(w)=0_Z+0_Z=0_Z$, e $T(\lambda v)=\lambda T(v)=\lambda 0_Z=0_Z$, quindi $v+w,\lambda v\in\Ker T$.
	\item Siano $y,z\in\Imm T$ e $\mu\in K$.
		Esistono due vettori $a,b\in V$ per i quali $T(a)=y$ e $T(b)=z$.
		Allora $z+y=T(a)+T(b)=T(a+b)$, cioè $z+y$ è l'immagine di un elemento $a+b$, che appartiene ancora a $V$; analogamente $T(\mu a)=\mu T(a)=\mu y$, quindi $\mu y$ è a sua volta l'immagine di un elemento $\mu a\in V$.
		Di conseguenza $z+y$ e $\mu y$ appartengono ancora a $\Imm T$.
\end{itemize}

\begin{osservazione} \label{o:applicazioni-sottospazi}
	Le applicazioni lineari conservano, in generale, i rapporti tra sottospazi: dati due sottospazi $W\le V$ e $U\le Z$ e un'applicazione lineare $T\colon V\to Z$, allora $T(W)\leq \Imm T$; analogamente le controimmagini di $U\le \Imm T$ sono $T^{-1}(U)\leq V$.

	Ad esempio, siano $y_1,y_2\in T(W)$.
	Essi certamente hanno le loro controimmagini in $W$: siano esse $w_1,w_2\in W$, per le quali $T(w_1)=y_1$ e $T(w_2)=y_2$.
	Allora per $\lambda,\mu\in K$ risulta $T(W)\ni T(\lambda w_1+\mu w_2)=\lambda T(w_1)+\mu T(w_2)=\lambda y_1+\mu y_2$, poiché $\lambda w_1+\mu w_2\in W$: di conseguenza $T(W)\le\Imm T$ (ma anche $T(W)\le Z$).
\end{osservazione}

\begin{teorema}
	Siano $T\in\lin(V,Z)$ e $\mathcal E=\{e_i\}_{i\in I}\subset V$.
	\begin{enumerate}
		\item Se $\mathcal E$ è un sistema di generatori per $V$ e $T$ è suriettiva, allora $\{T(e_i)\}_{i\in I}$ genera $Z$;
		\item Se $\mathcal E$ è linearmente indipendente in $V$ e $T$ è iniettiva, allora $\{T(e_i)\}_{i\in I}$ è linearmente indipendente in $Z$.
	\end{enumerate}
\end{teorema}
\begin{proof}
	\begin{enumerate}
		\item Sia $z\in Z$ e $T$ suriettiva: allora esiste $v\in V$ per cui $T(v)=z$.
			Dato che $\mathcal E$ genera $V$, possiamo scrivere $v$ come una combinazione lineare $v=\lambda_1e_1+\lambda_2e_2+\dots+\lambda_re_r$, con $\lambda_i\in K$ e $\{e_1,e_2,\dots,e_r\}\subset\mathcal E$.
			Risulta allora $z=T(v)=\lambda_1T(e_i)+\dots+\lambda_rT(e_r)$, cioè $\{T(e_1),\dots,T(e_r)\}$ è un sistema di generatori per $Z$.
		\item Sia ora $T$ iniettiva e $I_0$ un sottoinsieme di cardinalità finita di $I$.
			Consideriamo una combinazione lineare dei $T(e_i)$ nulla: per linearità
			\begin{equation*}
				0_Z=\sum_{j\in I_0}\lambda_jT(e_j)=T\bigg(\sum_{j\in I_0}\lambda_je_j\bigg).
			\end{equation*}
			Questo significa che $\sum_{j\in I_0}\lambda_je_j\in\Ker T$, ma poiché $T$ è iniettivo il suo nucleo è composto dal solo $0_V$.
			Allora $\sum_{j\in I_0}\lambda_je_j=0_V$, e dato che $\{e_i\}_{i\in I_0}$ è un sistema linearmente indipendente ciò può accadere se e solo se $\forall j\in I_0$ $\lambda_j=0_K$.
			Quindi $\{T(e_j)\}_{j\in I}$ è un sistema linearmente indipendente.\qedhere
	\end{enumerate}
\end{proof}
Combinando le due affermazioni, risulta che l'immagine di una base tramite un isomorfismo è una base dello spazio di arrivo.

\paragraph{Unicità di un'applicazione lineare}
Dato che ogni spazio vettoriale è descritto completamente da una sua base, che ne codifica tutte le proprietà utili, ci aspettiamo un risultato simile anche per le applicazioni lineari.
Nel seguente teorema vediamo infatti che ogni applicazione lineare è univocamente determinata dalla sua azione su una base.
\begin{teorema} \label{t:unicita-applicazione-lineare}
	Siano $T,\tilde T\colon V\to Z$ due applicazioni lineari, e $\{e_1,\dots,e_n\}$ una base di $V$.
	Se $T(e_i)=\tilde T(e_i)$ per ogni $i=1,\dots,n$, allora $T$ e $\tilde T$ coincidono su tutto $V$.
\end{teorema}
\begin{proof}
	Per linearità si ha, $\forall v\in V$,
	\begin{equation}
		T(v)=T\bigg(\!\sum_{j=1}^nv_je_j\bigg)=\sum_{j=1}^nv_jT(e_j)=\sum_{j=1}^nv_j\tilde T(e_j)=\tilde T(v)
	\end{equation}
	quindi le due applicazioni coincidono.
\end{proof}
Notare che questo risultato non dipende in alcun modo dalla base dello spazio di arrivo, o dalle specifiche immagini degli elementi $e_i$ della base dello spazio di partenza.

\section{Teoremi degli isomorfismi}
\begin{teorema}
	Sia $T\in\lin(V,Z)$, e fissiamo $z\in \Imm T$.
	Preso $x\in V$ tale che $T(x)=z$, la classe di equivalenza $\Ker T+x$ è l'insieme degli elementi $y$ di $V$ che sono controimmagini di $z$:
	\begin{equation*}
		 \Ker T+x=\{y\in V\colon T(y)=z\}.
	\end{equation*}
\end{teorema}
\begin{proof}
	Sia $v\in\Ker T+x$, cioè $v=\alpha+x$ per un certo $\alpha\in\Ker T$.
	La sua immagine è $T(v)=T(\alpha)+T(x)=0_Z+T(x)=T(x)=z$, quindi $v\in\{y\in V\colon T(y)=z\}$ ossia $\Ker T+x\subseteq\{y\in V\colon T(y)=z\}$.

	Sia invece $w\in\{y\in V\colon T(y)=z\}$: possiamo scriverlo come $w=\beta+x$ per un certo $\beta\in V$.
	Allora $T(w)=T(\beta)+T(x)$, ma $T(w)=T(x)$ per come è scelto $w$ quindi $T(\beta)=T(w)-T(x)=0$ poich\'e $T(w)=T(x)=z$.
	Di conseguenza $\beta\in\Ker T$, ossia $w\in\Ker T+x$ vale a dire $\{y\in V\colon T(y)=z\}\subseteq\Ker T+x$.

	Quindi, poiché i due insiemi si includono a vicenda, devono coincidere.
\end{proof}


\begin{teorema}[Primo teorema dell'isomorfismo] \label{t:isomorfismo-1}
	Siano $V,Z$ spazi vettoriali sul campo $K$, $T\colon V\to Z$ un'applicazione lineare e $\pi\colon V\to V\quot\Ker T$ la proiezione canonica $x\mapsto\Ker T+x$.
	Esiste ed è unica un'applicazione lineare iniettiva $\tilde{T}\in\lin(V\quot\Ker T,Z)$ tale che $\tilde{T}\circ\pi=T$.
	Inoltre
	\begin{equation}
		V\quot\Ker T\cong T(V).
		\label{eq:isomorfismo-1}
	\end{equation}
\end{teorema}
Un altro modo di enunciare il teorema è affermare che $\tilde{T}$ rende commutativo il grafico seguente.

\centerline{\xymatrix{V \ar[r]^T \ar[d]_\pi & Z \\ V\quot\Ker T \ar[ur]_{\tilde T}}}

\begin{proof}
	L'applicazione $\tilde{T}$ è naturalmente definita come $\tilde{T}(\Ker T+x)=T(x)$.
	Verifichiamo innanzitutto che essa sia \emph{ben definita}, ossia che $\tilde{T}(X)$ non dipenda dal rappresentante scelto per la classe $X$.
	Sia $\Ker T+x\in V\quot\Ker T$, e prendiamo un altro rappresentante $y\in V$, tale che $\Ker T+x=\Ker T+y$: le immagini delle due classi dovranno coincidere.
	Dato che $x\sim y$, risulta $x-y\in\Ker T$, ma allora $\tilde{T}(\Ker T+x)-\tilde{T}(\Ker T+y)=T(x)-T(y)=T(x-y)=0_Z$ cioè $\tilde{T}(\Ker T+x)=\tilde{T}(\Ker T+y)$.
	Analogamente, se per un $w\in V$ si avesse $\Ker T+x\ne\Ker T+w$, allora anche $\tilde{T}(\Ker T+x)\ne\tilde{T}(\Ker T+w)$: è sufficiente notare che se, per assurdo, le immagini coincidessero, allora dovrebbero coincidere anche le preimmagini, poich\'e risulterebbe $x-w\in\Ker T$.
	Quindi l'applicazione è ben definita.

	L'unicità è immediata: supponiamo che esista un'altra applicazione $U\colon V\quot\Ker T\to Z$ tale che $U\big(\pi(x)\big)=T(x)$.
	Per ogni $y\in V\quot\Ker T$ esiste $x\in V$ tale per cui $\pi(x)=y$, dunque $U(y)=U\big(\pi(x)\big)=T(x)=\tilde{T}\big(\pi(x)\big)=\tilde{T}(y)$, ma allora $U=\tilde{T}$.

	Mostriamo ora che è lineare, rispetto alle operazioni nello spazio quoziente introdotte nella sezione \ref{sec:spazi_quoziente}.
	Prendiamo $\Ker T+a,\Ker T+b\in V\quot\Ker T$ e $\lambda\in K$: risulta
	\begin{multline}
		\tilde{T}\big((\Ker T+a)+\lambda(\Ker T+b)\big)=\tilde{T}\big(\!\Ker T+(a+\lambda b)\big)=T(a+\lambda b)=\\=T(a)+\lambda T(b)=\tilde{T}(\Ker T+a)+\lambda\tilde{T}(\Ker T+b)
	\end{multline}
	perciò $\tilde{T}$ è lineare.

	Il nucleo di $\tilde{T}$ è $\Ker\tilde{T}=\{\Ker T+v\in V/\Ker T\colon T(v)=0_Z\}$.
	Ma se $T(v)=0_Z$ allora $v\in\Ker T$, perciò $\Ker T+v=\Ker T+0_V$ ossia $\Ker\tilde{T}=\{\Ker T+0_V\}$: ciò prova che $\tilde{T}$ è iniettiva.

	Se infine prendiamo l'applicazione $T$ solo tra $V$ e la sua immagine $T(V)$, è ovviamente suriettiva.
	Possiamo quindi trovare un'unica applicazione $L\colon V\quot\Ker T\to T(V)$ sfruttando quanto dimostrato finora.
	Se $z\in T(V)$ allora esiste $v\in V$ tale che $T(v)=z$, dunque è sufficiente prendere $a=\Ker T+v$ per ottenere $L(a)=z$, perciò per ogni $z\in T(V)$ troviamo sempre una controimmagine in $L$: ciò prova che $L\colon V\quot\Ker T\to T(V)$ è anche suriettiva, oltre che iniettiva.
	
	Di conseguenza $L\colon V\quot\Ker T\to T(V)$ è un isomorfismo.
\end{proof}

\begin{corollario} \label{cor:nullita-rango}
	Se $V$ ha dimensione finita e $T\in\lin(V,Z)$, allora $\dim V=\dim\Ker T+\dim\Imm T$.
\end{corollario}
\begin{proof}
	Segue facilmente dal teorema precedente e dal \ref{t:dimensione-quoziente}.
\end{proof}
Da quest'ultima formula possiamo dedurre delle proprietà generali su nucleo e immagine di applicazioni vettoriali.
Se $T\in\lin(V,Z)$, distinguiamo alcuni casi.
\begin{itemize}
	\item Se $\dim V>\dim Z$, allora $T$ non può essere iniettiva, infatti $\Imm T\le Z$ implica $\dim\Imm T\le\dim Z$ da cui $\dim\Ker T=\dim V-\dim\Imm T>0$.
	\item Se $\dim V<\dim Z$, allora $T$ non può essere suriettiva, dato che $\dim\Imm T=\dim V-\dim\Ker T\le\dim V<\dim Z$ quindi non può risultare $\Imm T=Z$.
	\item Se $\dim V=\dim Z$, come è per gli endomorfismi (in cui $Z=V$), iniettività e suriettività si implicano a vicenda: se $T$ è iniettiva allora $\dim\Ker T=0$ da cui $\dim\Imm T=\dim Z$, cioè $\Imm T=Z$ quindi $T$ è suriettiva.
		Viceversa, se $T$ è suriettiva allora $\dim\Imm T=\dim Z=\dim V$ da cui $\dim\Ker T=0$ ossia $\Ker T=\{0\}$.
\end{itemize}

\begin{teorema}[Secondo teorema dell'isomorfismo]
	Siano $W,Z$ sottospazi vettoriali di $V$.
	Allora l'applicazione $T\colon (W+Z)\quot Z\to W\quot(W\cap Z)$ definita come
	\begin{equation}
		T(Z+w+z)=W\cap Z+w
	\end{equation}
	è un isomorfismo.
\end{teorema}
\begin{proof}
	Innanzitutto l'applicazione $T$ deve essere ben definita, cioè non deve dipendere dai rappresentanti della classe.
	Prendiamo due elementi $Z+w+z$ e $Z+w'+z'$ (per $z,z'\in Z$ e $w,w'\in W$) i cui rappresentanti sono in relazione tra loro, ossia tali che $w+z-w'-z'\in Z$.
	Chiaramente $z-z'\in Z$, da cui segue che $w-w'\in Z$: ma allora $w-w'\in W\cap Z$ quindi le immagini $W\cap Z+w$ e $W\cap Z+w'$ coincidono (la loro differenza è $W\cap Z+w-W\cap Z+w'=W\cap Z+(w-w')=W\cap Z$ che è l'elemento neutro dello spazio di arrivo).
	Questo prova che $T$ è ben definita.
	
	Verifichiamo che è lineare: presi $z_1,z_2\in Z$, $w_1,w_2\in W$ e $\lambda\in K$ abbiamo\footnote{Nel tornare da $W\cap Z+w$ a $Z+w+z$, la scelta dell'elemento $z$ è arbitraria, poich\'e come elementi dello spazio quoziente $Z+z=Z$ per ogni $z\in Z$, quindi possiamo scegliere proprio $z_1$ e $z_2$ per riottenere le controimmagini.}
	\begin{equation}
		\begin{split}
			T\big(Z+w_1+z_1+\lambda(Z+w_2+z_2)\big)&=T(Z+z_1+\lambda z_2+w_1+\lambda w_2)=\\
			&=W\cap Z+w_1+\lambda w_2=\\
			&=W\cap Z+w_1+\lambda(W\cap Z+w_2)=\\
			&=T(Z+w_1+z_1)+\lambda T(Z+w_2+z_2)
		\end{split}
	\end{equation}
	quindi $T$ è lineare.

	Evidentemente $T$ è suriettiva.
	Il suo nucleo è formato da quegli elementi $Z+w+z$ tali che $W\cap Z+w=0$, ossia $w\in W\cap Z$.
	Allora vale anche $w\in Z$, da cui $w+z\in Z$.
	Di conseguenza $Z+w+z=Z$, cioè è lo zero dello spazio quoziente $(W+Z)\quot Z$.
	Ciò prova che $T$ è anche iniettiva, quindi è un isomorfismo.
\end{proof}
\begin{corollario}[Formula di Grassmann]
	Se gli spazi vettoriali $W$ e $Z$ hanno dimensione finita, allora $\dim(W+Z)=\dim W+\dim Z-\dim(W\cap Z)$.
\end{corollario}
\begin{proof}
	I due spazi $(W+Z)\quot Z$ e $W\quot(W\cap Z)$ dal teorema precedente sono isomorfi, perciò hanno la stessa dimensione.
	Allora per il teorema \ref{t:dimensione-quoziente} si ha che $\dim(W+Z)-\dim Z=\dim W-\dim(W\cap Z)$, da cui la tesi.
\end{proof}

\section{Matrice associata}
\label{sec:matrice-associata}
Abbiamo visto nel capitolo precedente che per uno spazio vettoriale $V$ su un campo $K$ e di dimensione $n$, una volta scelta una base, possiamo associare ad ogni $v\in V$ un vettore in $K^n$, che contiene le sue coordinate rispetto alla base.
Studiando le applicazioni lineari su $V$ è utile quindi, in modo analogo, associare ad ogni applicazione un opportuno oggetto matematico che operi sui vettori delle coordinate.
In questo modo possiamo l'applicazione di una mappa lineare su un vettore diventa un calcolo concreto.
Prendiamo un'applicazione lineare $T\colon V\to Z$, con $V$ e $Z$ sul campo $K$, e le basi $\mathcal B=\{e_1,\dots,e_n\}$ per $V$ e $\mathcal C=\{f_1,\dots,f_m\}$ per $Z$.
Abbiamo visto che l'azione di un'applicazione lineare è univocamente determinata dalla sua azione su una base dello spazio di partenza: questo implica che $T$ è completamente descritta dalle immagini $T(e_i)$ per $i=1,\dots,n$.
Come elementi di $Z$, queste immagini dei vettori della base di $V$ si scrivono in termini della base $\mathcal C$ come combinazioni lineari\footnote{
	L'ordine degli indici nell'equazione si può capire con la notazione simbolica del prodotto tra il vettore colonna $A_{ij}$ e il ``vettore riga'' degli $f_i$.
}
\begin{equation}
	T(e_i)=\sum_{k=1}^mA_{ki}f_k=
	\begin{pmatrix}
		f_1&\cdots&f_m
	\end{pmatrix}
	\begin{pmatrix}
		A_{1i}\\
		\vdots\\
		A_{mi}
	\end{pmatrix}
\end{equation}
in cui i coefficienti $A_{1i},\dots,A_{mi}$ formano il vettore delle coordinate di $T(e_i)$ nella base $\mathcal C$.
Per $i$ che scorre da 1 a $n$, affiancando le colonne troviamo che
\begin{equation}
	\begin{pmatrix}
		T(e_1)&\cdots&T(e_n)
	\end{pmatrix}
	=
	\begin{pmatrix}
		f_1&\cdots&f_m
	\end{pmatrix}
	A
\end{equation}
dove abbiamo definito la matrice
\begin{equation}
	A\defeq
	\begin{pmatrix}
		A_{11}&A_{12}&\cdots&A_{1n}\\
		A_{21}&A_{22}&\cdots&A_{2n}\\
		\vdots&\vdots&\ddots&\vdots\\
		A_{m1}&A_{m2}&\cdots&A_{nm}
	\end{pmatrix}.
	\label{eq:matrice-associata}
\end{equation}
che è detta \emph{matrice associata} all'applicazione $T$, nelle basi $\mathcal B$ per $V$ e $\mathcal C$ per $Z$.
Riassumiamo questo procedimento nella seguente definizione.
\begin{definizione} \label{d:matrice-associata}
	Sia $T\colon V\to Z$ lineare, con $V$ e $Z$ sul medesimo campo $K$, e si prendano le basi $\mathcal B=\{e_1,\dots,e_n\}$ e $\mathcal C=\{f_1,\dots,f_m\}$ per $V$ e $Z$ rispettivamente.
	Se per $i=1,\dots,n$ si ha
	\begin{equation}
		T(e_i)=\sum_{k=1}^mA_{ki}f_k
	\end{equation}
	allora la matrice
	\begin{equation}
		A=
		\begin{pmatrix}
			A_{11}&\cdots&A_{1n}\\
			\vdots&\ddots&\vdots\\
			A_{m1}&\cdots&A_{nm}
		\end{pmatrix}
		\in\mat(m,n,K)
	\end{equation}
	è detta \emph{matrice associata} a $T$, nelle basi scelte.
\end{definizione}
In breve, per costruire la matrice associata bisogna individuare i $nm$ coefficienti $A_{ki}$ tali che, per $i=1,\dots,n$, si ha $T(e_i)=\sum_{k=1}^mA_{ki}f_k$ (mantenendo il corretto ordine degli indici), ossia il sistema
\begin{equation}
	\begin{cases}
		T(e_1)=A_{11}f_1+A_{21}f_2+\cdots+A_{m1}f_m\\
		\vdots\\
		T(e_n)=A_{1n}f_1+A_{2n}f_2+\cdots+A_{mn}
	\end{cases},
\end{equation}
prendere la matrice del sistema di equazioni (la cui $i$-esima riga è formata dai coefficienti $A_{ki}$ nella $i$-esima riga del sistema), e prenderne la trasposta.

Per un generico vettore $v=\sum_{j=1}^nv_je_j\in V$ si ha allora
\begin{equation}
	T(v)=\sum_{j=1}^nv_jT(e_j)=\sum_{j=1}^n\sum_{k=1}^mv_jA_{kj}f_k
\end{equation}
ma scambiando le somme possiamo vedere questa equazione come
\begin{equation}
	T(v)=\sum_{k=1}^mf_k\sum_{j=1}^nA_{kj}v_j=
	\begin{pmatrix}
		f_1&\cdots&f_m
	\end{pmatrix}
	A
	\begin{pmatrix}
		v_1\\
		\vdots\\
		v_n
	\end{pmatrix}
	=
	\begin{pmatrix}
		f_1&\cdots&f_m
	\end{pmatrix}
	\begin{pmatrix}
		T(v)_1\\
		\vdots\\
		T(v)_m
	\end{pmatrix}
\end{equation}
da cui ricaviamo
\begin{equation}
	\begin{pmatrix}
		T(v)_1\\
		\vdots\\
		T(v)_m
	\end{pmatrix}
	=A
	\begin{pmatrix}
		v_1\\
		\vdots\\
		v_n
	\end{pmatrix}.
\end{equation}
L'azione della mappa $T$ su $v\in V$ si traduce quindi nella moltiplicazione della matrice $A$ cos\`i definita per il vettore delle coordinate $[v]_{\mathcal B}$, o in breve
\begin{equation}
	[T(v)]_{\mathcal C}=A[v]_{\mathcal B}.
\end{equation}
Ad ogni applicazione $T$ da uno spazio di dimensione $n$ a uno spazio di dimensione $m$, sul campo $K$, possiamo allora associare una matrice in $\mat(m,n,K)$.

Vediamo questo fatto in maniera rigorosa nel seguente teorema.
Anzich\'e considerare lo spazio delle applicazioni lineari tra due generici spazi $V$ e $Z$ su un campo $K$, partiremo direttamente dalle applicazioni lineari tra $K^n$ e $K^m$ (dove $n=\dim V$ e $m=\dim Z$): l'isomorfismo $\lin(K^n,K^m)\cong\lin(V,Z)$ dovrebbe risultare ovvio.
\begin{teorema} \label{t:matrice-associata}
	La mappa $T\colon\mat(m,n,K)\to\lin(K^n,K^m)$ definita da $T\colon A\to T_A$ dove $T_A$ è l'applicazione lineare
	\begin{equation}
		T_A\colon
		\begin{bmatrix}
			v_1\\
			\vdots\\
			v_n
		\end{bmatrix}
		\in K^n\to
		\begin{bmatrix}
			\sum_{i=1}^nA_{1i}v_i\\
			\vdots\\
			\sum_{i=1}^nA_{mi}v_i
		\end{bmatrix}
		\in K^m
		\label{eq:azione-applicazione-lineare-associata}
	\end{equation}
	è un'isomorfismo tra i due spazi vettoriali.
\end{teorema}
\begin{proof}
	Mostriamo innanzitutto che la mappa è lineare: date $A,B\in\mat(m,n,K)$ si ha, per il vettore $v=[v_1\,\cdots\,v_n]^T$ come nella \eqref{eq:azione-applicazione-lineare-associata},
	\begin{multline}
		T_{A+B}(v)=
		\begin{bmatrix}
			\sum_{i=1}^n(A+B)_{1i}v_i\\
			\vdots\\
			\sum_{i=1}^n(A+B)_{mi}v_i
		\end{bmatrix}
		=
		\begin{bmatrix}
			\sum_{i=1}^nA_{1i}v_i+\sum_{i=1}^nB_{1i}v_i\\
			\vdots\\
			\sum_{i=1}^nA_{mi}v_i+\sum_{i=1}^nB_{mi}v_i\\
		\end{bmatrix}
		=\\=
		\begin{bmatrix}
			\sum_{i=1}^nA_{1i}v_i\\
			\vdots\\
			\sum_{i=1}^nA_{mi}v_i
		\end{bmatrix}
		+
		\begin{bmatrix}
			\sum_{i=1}^nB_{1i}v_i\\
			\vdots\\
			\sum_{i=1}^nB_{mi}v_i
		\end{bmatrix}
		=T_A(v)+T_B(v)
		\label{eq:additivita-matrice-associata}
	\end{multline}
	perciò è additiva, e analogamente è omogenea poich\'e
	\begin{equation}
		T_{\lambda A}(v)=
		\begin{bmatrix}
			\sum_{i=1}^n(\lambda A)_{1i}v_i\\
			\vdots\\
			\sum_{i=1}^n(\lambda A)_{mi}v_i
		\end{bmatrix}
		=
		\begin{bmatrix}
			\lambda\sum_{i=1}^nA_{1i}v_i\\
			\vdots\\
			\lambda\sum_{i=1}^nA_{mi}v_i
		\end{bmatrix}
		=\lambda
		\begin{bmatrix}
			\sum_{i=1}^nA_{1i}v_i\\
			\vdots\\
			\sum_{i=1}^nA_{mi}v_i
		\end{bmatrix}
		=\lambda T_A(v).
		\label{eq:omogeneita-matrice-associata}
	\end{equation}
	Di conseguenza $T$ è lineare.

	Il nucleo di $T$ è composto dalle matrici $A\in\mat(m,n,K)$ tali che $T_A$ è l'applicazione identicamente nulla, cioè $T_A(v)=0$ $\forall v\in V$.
	Presi gli elementi $\{e_1,\dots e_n\}$ della base canonica di $K^n$ si ha allora, dato che la $j$-esima componente di $e_i$ è $(e_i)_j=\delta_{ij}$, che
	\begin{equation}
		T_A(e_k)=
		\begin{bmatrix}
			A_{1k}\\
			\vdots\\
			A_{mk}
		\end{bmatrix}
	\end{equation}
	per il $k$-esimo elemento: il vettore coincide con la $k$-esima colonna della matrice $A$.
	Se l'applicazione $T_A$ è identicamente nulla, $T_A(e_k)=0$ $\forall k\in\{1,\dots,n\}$ ma ciò significa che $A$ ha tutti gli elementi nulli, cioè $A=0$.
	Dunque $T$ è iniettiva.

	Presa ora un'applicazione lineare $L\in\lin(K^n,K^m)$, la sua azione sulla base canonica $\{e_1,\dots,e_n\}$ di $K^n$ definisce $n$ vettori di $K^m$.
	Affiancando le colonne di questi vettori costruiamo la matrice $A_L\defeq[L(e_1)\,\cdots L(e_n)]\in\mat(m,n,K)$.
	A tale matrice, attraverso la mappa $T$, associamo dunque l'applicazione lineare $T_L$ come nella \eqref{eq:azione-applicazione-lineare-associata}, ed è evidente che per ogni $k\in\{1,\dots,n\}$ si ha $L(e_k)=A_Le_k=T_L(e_k)$, per come è stata costruita $A_L$.
	Siccome $L$ e $T_L$ coincidono su una base di $K^n$, per il teorema \ref{t:unicita-applicazione-lineare} le due applicazioni coincidono su tutto $K^n$.
	Per ogni applicazione $L\in\lin(K^n,K^m)$ possiamo cos\`i individuare una matrice $A_L\in\mat(m,n,K)$ tale che $T\colon A_L\to L$, e ciò mostra che $T$ è suriettiva.

	La mappa $T$ è quindi un isomorfismo tra $\mat(m,n,K)$ e $\lin(K^n,K^m)$.
\end{proof}

\section{Composizione}
Prendiamo tre spazi vettoriali $V,W,Z$ sullo stesso campo $K$: prese due applicazioni lineari $T\colon V\to W$ e $S\colon W\to Z$, possiamo comporli (come qualunque funzione) per ottenere un'applicazione $S\circ T\colon V\to Z$, che è ancora evidentemente lineare: per $a,b\in K$ e $v_1,v_2\in V$ si ha
\begin{equation}
	(S\circ T)(av_1+bv_2)=S\big(aT(v_1)+bT(v_2)\big)=S\big(aT(v_1)\big)+S\big(bT(v_2)\big)=a(S\circ T)(v_1)+b(S\circ T)(v_2).
	\label{eq:linearita-applicazione-composta}
\end{equation}
Anche all'applicazione composta possiamo associare una matrice di $\mat(\dim Z,\dim V,K)$ come già visto.
Vediamo però come tale matrice si mette in relazione con le singole matrici di $S$ e $T$.
\begin{teorema} \label{t:matrice-applicazione-composta}
	Siano $V$, $W$ e $Z$ spazi vettoriali sul campo $K$, in cui scegliamo le basi, rispettivamente, $\{e_1,\dots,e_n\}$, $\{f_1,\dots,f_m\}$, $\{g_1,\dots,g_p\}$.
	Se $T\colon V\to W$ e $S\colon W\to Z$ sono applicazioni lineari tra gli spazi e $A_T\in\mat(m,n,K)$ e $A_S\in\mat(p,m,K)$ le matrici associate, nelle basi date, rispettivamente alle due applicazioni, allora la matrice associata a $S\circ T$ è il prodotto $A_SA_T\in\mat(p,n,K)$, sempre nelle basi scelte.
\end{teorema}
\begin{proof}
	Applicando la composizione di $S$ e $T$ all'$i$-esimo elemento della base di $V$ otteniamo una combinazione lineare
	\begin{equation}
		(S\circ T)(e_i)=\sum_{k=1}^p(A_{S\circ T})_{ki}g_k
	\end{equation}
	ma allo stesso tempo
	\begin{multline}
		S\big(T(e_i)\big)=S\bigg(\sum_{j=1}^m(A_T)_{ji}f_j\bigg)=\sum_{j=1}^m(A_T)_{ji}S(f_j)=\sum_{j=1}^m(A_T)_{ji}\sum_{k=1}^p(A_S)_{kj}g_k=\sum_{k=1}^p\sum_{j=1}^m(A_S)_{kj}(A_T)_{ji}g_k.
	\end{multline}
	Per l'unicità della matrice associata si ha cos\`i
	\begin{equation}
		(A_{S\circ T})_{ki}=\sum_{j=1}^m(A_S)_{kj}(A_T)_{ji}
	\end{equation}
	che è il prodotto riga per colonna, in ordine, $A_SA_T$.
\end{proof}
Abbiamo visto che lo spazio $\End(V)$ delle applicazioni lineari da $V$ (sul campo $K$, con $\dim V=n$) a s\'e stesso è un'algebra: quest'ultimo teorema mostra in aggiunta che la mappa che associa a una matrice, in questo caso quadrata, $\mat(n,K)$ un'applicazione $T\in\End(V)$ (come nella \eqref{eq:azione-applicazione-lineare-associata}) preserva anche l'operazione di composizione, vale a dire che la composizione di due applicazioni corrisponde al prodotto delle matrici associate.
La matrice associata all'applicazione identità di $V$ è, in qualsiasi base, inoltre la matrice $n\times n$ diagonale
\begin{equation}
	I=
	\begin{bmatrix}
		1&0&\cdots&0\\
		0&1&\cdots&0\\
		\vdots&\vdots&\ddots&\vdots\\
		0&0&\cdots&1
	\end{bmatrix}
\end{equation}
ossia $I_{ij}=\delta_{ij}$, che è l'elemento neutro di $\mat(n,K)$.
In altre parole, ciò mostra l'isomorfismo \emph{tra algebre} $\mat(n,K)\cong\End(V)$.

\section{Inversione}
Guardiamo ora al problema di determinare quando un'applicazione è invertibile.
Prendiamo due spazi vettoriali $V$ e $Z$ sul campo $K$ e un'applicazione lineare $T\colon V\to Z$.
Come una qualsiasi funzione, $T$ è invertibile se e solo se è iniettiva, quindi in questo caso se $\Ker T=\{0\}$.
Se anche $\Ker T\ne\{0\}$ possiamo invertire $T$ restringendo il suo dominio a un sottospazio di $V$ che non comprenda $\Ker T$.

Prendiamo invece lo spazio degli endomorfismi di $V$, a ciascuno dei quali possiamo associare una matrice quadrata $n\times n$ ($n=\dim V$).
In questo caso sappiamo quando tale matrice è invertibile, ossia se esiste una matrice (sempre $n\times n$) $B$ tale che $AB=BA=I$.
Nel seguente teorema vediamo come questa condizione si collega all'invertibilità della corrispondente applicazione lineare: ricordiamo che, essendo un endomorfismo, se è iniettivo è anche suriettivo, dunque è un isomorfismo.
\begin{teorema} \label{t:invertibilita-matrice-associata}
	Sia $T\in\End(V)$ e $A_T$ la sua matrice associata, nella stessa base in partenza e in arrivo.
	La matrice $A_T$ è invertibile se e solo se $T$ è un isomorfismo.
\end{teorema}
\begin{proof}
	Sia $T$ un isomorfismo, cioè invertibile: esiste quindi $T^{-1}\in\End(V)$; prendiamo le rispettive matrici associate $A_T,A_{T^{-1}}\in\mat(n,K)$, dove $K$ è il campo dello spazio vettoriale $V$ e $n$ la sua dimensione.
	Poich\'e $T^{-1}\circ T=T\circ T^{-1}=\id_V$, l'applicazione identità di $V$, per il teorema \ref{t:matrice-applicazione-composta} si ha
	\begin{equation}
		I=A_TA_{T^{-1}}=A_{T^{-1}}A_T
	\end{equation}
	cioè, per definizione, $A_T$ è invertibile, e inoltre $A_{T^{-1}}=A_T^{-1}$.

	Viceversa, sia ora $A_T$ invertibile: alla sua inversa $A_T^{-1}$ possiamo associare un'applicazione lineare $L$ tale che
	\begin{equation}
		L(e_i)=\sum_{k=1}^n(A_T^{-1})_{ki}e_k
	\end{equation}
	dove $\{e_1,\dots,e_n\}$ è la base di $V$.
	Allora l'applicazione $L\circ T$ è tale che
	\begin{equation}
		(L\circ T)(e_i)=\sum_{k=1}^n(A_T)_{ki}L(e_k)=\sum_{k=1}^n\sum_{j=1}^n(A_T)_{ki}(A_T^{-1})_{jk}e_j=\sum_{j=1}^n\delta_{ji}e_j=e_i
	\end{equation}
	dato che
	\begin{equation}
		\sum_{k=1}^n(A_T^{-1})_{jk}(A_T)_{ki}=\delta_{ji}
	\end{equation}
	essendo le due matrici l'una l'inversa dell'altra, cioè $(A_T^{-1}A_T)_{ji}=I_{ji}=\delta_{ji}$.
	Abbiamo trovato quindi che $L\circ T=\id_V$, e analogamente si mostra che $T\circ L=\id_V$.
	Ciò prova che l'inverso di $T$ esiste (è $L$), cioè che $T$ è un isomorfismo.
\end{proof}
Un endomorfismo invertibile è detto \emph{automorfismo}, e l'insieme degli automorfismi di uno spazio $V$ si indica con $\Aut(V)$.
Non entreremo ora nei dettagli su come costruire l'applicazione inversa: per farlo ci serve la teoria dei determinanti, che vedremo in seguito.
\section[Cambiamento di base]{Cambiamento di base\footnote{
	La trattazione di questo argomento differisce da quanto visto a lezione: ho evitato qui una dimostrazione formale, con le sommatorie delle varie componenti di matrici e vettori, preferendo invece una trattazione più ``figurativa'' (tratta da \cite{multilinear-algebra}), mostrando i vettori in colonne e righe.
	Il contenuto di fondo, ovviamente, è lo stesso: questo è solo un modo meno rigoroso.
}}
\label{sec:cambiamento-base}
Prendiamo due basi $\mathcal B=\{e_1,\dots,e_n\}$ e $\tilde{\mathcal B}=\{\tilde e_1,\dots\tilde e_n\}$ in uno spazio vettoriale $V$ di dimensione $n$, finita.
Vogliamo studiare in questa sezione come cambiano i vettori delle coordinate e le matrici associate passando da una base all'altra.
Gli elementi della base $\tilde{\mathcal B}$, essendo elementi di $V$, si scrivono a loro volta come combinazione lineare nella base $\mathcal B$: per ogni $\tilde e_i$ esistono dei coefficienti $L_{ik}$ tali che
\begin{equation}
	\tilde e_i=\sum_{k=1}^nL_{ik}e_k=
	\begin{pmatrix}
		e_1&\cdots&e_n
	\end{pmatrix}
	\begin{pmatrix}
		L_{i1}\\
		\vdots\\
		L_{in}
	\end{pmatrix}.
\end{equation}
Affiancando tutti gli elementi della base $\tilde{\mathcal B}$ scriviamo allora che
\begin{equation}
	\begin{pmatrix}
		\tilde e_1&\cdots&\tilde e_n
	\end{pmatrix}
	=
	\begin{pmatrix}
		e_1&\cdots&e_n
	\end{pmatrix}
	L
	\label{eq:trasformazione-base}
\end{equation}
dove $L$ è la matrice
\begin{equation}
	\begin{pmatrix}
		L_{11}&\cdots&L_{n1}\\
		\vdots&\ddots&\vdots\\
		L_{1n}&\cdots&L_{nn}
	\end{pmatrix}
\end{equation}
la cui $j$-esima colonna contiene le coordinate nella base $\mathcal B$ del vettore $\tilde e_j$ della nuova base.
Tale matrice è associata, chiaramente, a un isomorfismo, poich\'e tutte le sue colonne sono linearmente indipendenti (sono una base per definizione) dunque essendo un endomorfismo iniettivo è automaticamente anche suriettivo; $L$ è dunque sempre invertibile.

Prendiamo ora un vettore $v\in V$, che scriviamo come combinazione lineare $\sum_{k=1}^nv_ke_k$ nella base $\mathcal B$ o anche, simbolicamente, con
\begin{equation}
	\begin{pmatrix}
		e_1&\cdots&e_n
	\end{pmatrix}
	\begin{pmatrix}
		v_1\\
		\vdots\\
		v_n
	\end{pmatrix}.
	\label{eq:vettore-coordinate-base-originale}
\end{equation}
Nell'altra base $\tilde{\mathcal B}$, esisteranno dei coefficienti $\tilde v_k$ tali che
\begin{equation}
	v=\sum_{k=1}^n\tilde v_k\tilde e_k=
	\begin{pmatrix}
		\tilde e_1&\cdots&\tilde e_n
	\end{pmatrix}
	\begin{pmatrix}
		\tilde v_1\\
		\vdots\\
		\tilde v_n
	\end{pmatrix},
\end{equation}
perciò usando la \eqref{eq:trasformazione-base} abbiamo
\begin{equation}
	v=
	\begin{pmatrix}
		\tilde e_1&\cdots&\tilde e_n
	\end{pmatrix}
	\begin{pmatrix}
		\tilde v_1\\
		\vdots\\
		\tilde v_n
	\end{pmatrix}=
	\begin{pmatrix}
		e_1&\cdots&e_n
	\end{pmatrix}
	L
	\begin{pmatrix}
		\tilde v_1\\
		\vdots\\
		\tilde v_n
	\end{pmatrix}
\end{equation}
da cui ricaviamo, uguagliandola alla \eqref{eq:vettore-coordinate-base-originale},
\begin{equation}
	\begin{pmatrix}
		v_1\\
		\vdots\\
		v_n
	\end{pmatrix}
	=L
	\begin{pmatrix}
		\tilde v_1\\
		\vdots\\
		\tilde v_n
	\end{pmatrix}
	\qqq
	\begin{pmatrix}
		\tilde v_1\\
		\vdots\\
		\tilde v_n
	\end{pmatrix}
	=L^{-1}
	\begin{pmatrix}
		v_1\\
		\vdots\\
		v_n
	\end{pmatrix}
	\label{eq:cambiamento-base-vettori-coordinate}
\end{equation}
dunque se $L$ è la matrice che porta la base $\mathcal B$ nella base $\tilde{\mathcal B}$ (secondo la \eqref{eq:trasformazione-base}) allora i vettori delle coordinate si trasformano con l'inversa della matrice, ossia $[v]_{\tilde{\mathcal B}}=L^{-1}[v]_{\mathcal B}$.
Si dice in questo caso che i vettori sono \emph{contravarianti}, poich\'e cambiano in modo ``contrario'' a come cambia la base.

Sia ora $T\colon V\to Z$ un'applicazione lineare, con $V$ di dimensione $n$ e $Z$ di dimensione $m$, definiti sul medesimo campo $K$.
Siano $\mathcal B,\tilde{\mathcal B}$ due basi di $V$ e $\mathcal C,\tilde{\mathcal C}$ due basi di $Z$.
Sia $A\in\mat(m,n,K)$ la matrice associata a $T$ nelle due basi $\mathcal B$ e $\mathcal C$, ossia tale che per $v\in V$
\begin{equation}
	[T(v)]_{\mathcal C}=A[v]_{\mathcal B}.
	\label{eq:matrice-associata-base-originale}
\end{equation}
Se $\tilde A$ è associata a $T$ nelle altre due basi, si ha equivalentemente
\begin{equation}
	[T(v)]_{\tilde{\mathcal C}}=\tilde A[v]_{\tilde{\mathcal B}}.
	\label{eq:matrice-associata-base-trasformata}
\end{equation}
Se ora $L$ è la matrice che porta la base $\mathcal B$ nella $\tilde{\mathcal B}$ e $M$ la matrice che porta $\mathcal C$ in $\tilde{\mathcal C}$ (come nella \eqref{eq:trasformazione-base}) allora sappiamo dalla \eqref{eq:cambiamento-base-vettori-coordinate} che
\begin{equation}
	L[v]_{\tilde{\mathcal B}}=[v]_{\mathcal B}
	\hspace{1cm}\text{e}\hspace{1cm}
	M[T(v)]_{\tilde{\mathcal C}}=[T(v)]_{\mathcal C}
\end{equation}
dunque inserendole nella \eqref{eq:matrice-associata-base-originale} troviamo
\begin{equation}
	M[T(v)]_{\tilde{\mathcal C}}=AL[v]_{\tilde{\mathcal B}}
	\qqq
	[T(v)]_{\tilde{\mathcal C}}=M^{-1}AL[v]_{\tilde{\mathcal B}}
\end{equation}
e per confronto con la \eqref{eq:matrice-associata-base-trasformata} nelle nuove basi otteniamo
\begin{equation}
	\tilde A=M^{-1}AL
	\label{eq:trasformazione-matrice-associata}
\end{equation}
che dà la matrice associata a $T$ nelle basi $\tilde{\mathcal B}$ in partenza e $\tilde{\mathcal C}$ in arrivo a partire dalla matrice associata nelle basi originarie $\mathcal B$ e $\mathcal C$.
In particolare, se $T\in\End(V)$, e scegliamo la stessa base sia in partenza che in arrivo, abbiamo che $\tilde A=L^{-1}AL$ è la matrice associata a $T$ nella nuova base.
Le matrici associate a un endomorfismo in basi differenti sono quindi tutte matrici simili.

Come esempio, prendiamo in $\R^2$ la base canonica $\mathcal B$ e la base
\begin{equation}
	\tilde{\mathcal B}=\left\{
		\begin{bmatrix}
			0\\
			-2
		\end{bmatrix},
		\begin{bmatrix}
			1\\
			2
		\end{bmatrix}
	\right\}
\end{equation}
e l'applicazione lineare $T\colon\R^2\to\R^2$ a cui è associata nella base canonica la matrice
\begin{equation}
	A=
	\begin{pmatrix}
		5&0\\
		1&4
	\end{pmatrix}.
\end{equation}
Per costruire la matrice del cambiamento di base, $L$, dobbiamo prendere le coordinate dei vettori di $\tilde{\mathcal B}$ rispetto alla base $\mathcal B$: poich\'e quest'ultima è la base canonica le coordinate coincidono con le componenti dei vettori stessi, perciò
\begin{equation}
	L=
	\begin{pmatrix}
		0&1\\
		-2&2
	\end{pmatrix}
\end{equation}
di conseguenza la matrice associata a $T$ nella base $\tilde{\mathcal B}$ (in arrivo e in partenza) è
\begin{equation}
	\tilde A=L^{-1}AL=
	\begin{pmatrix}
		1&-\frac12\\
		1&0
	\end{pmatrix}
	\begin{pmatrix}
		5&0\\
		1&4
	\end{pmatrix}
	\begin{pmatrix}
		0&1\\
		-2&2
	\end{pmatrix}
	=
	\begin{pmatrix}
		4&\frac12\\
		0&5
	\end{pmatrix}.
\end{equation}

